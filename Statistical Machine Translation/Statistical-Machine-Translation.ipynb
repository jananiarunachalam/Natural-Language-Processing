{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQFKBYPRAeT1"
   },
   "source": [
    "# Statistical Machine Translation\n",
    "\n",
    "<br>\n",
    "Corpora:\n",
    "<ol>\n",
    "<li>German – English</li>\n",
    "<li>French – English</li>\n",
    "</ol>\n",
    "\n",
    "<br>\n",
    "Implementing a function that will output a table containing the word translation probabilities that were learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33b2D4TxtfTW"
   },
   "outputs": [],
   "source": [
    "# ----- Importing libraries ----- \n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import tqdm\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DsyuNT92twz1",
    "outputId": "356f9240-35dd-4694-8aca-b347928fff5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# ----- Mounting drive to access data stored in the drive -----\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UwAwH7hltfTc"
   },
   "outputs": [],
   "source": [
    "# ----- Reading files & preprocessing the data -----\n",
    "# ----- Removing all punctuations except apostrophes ----- \n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = text.split(\"\\n\")\n",
    "    exclude = '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    text = [t.translate(str.maketrans('', '', exclude)).lower() for t in text]\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The language files are in the same directory as the colab file\n",
    "\n",
    "f = open('/content/drive/My Drive/NLP_Assignment_3/fr-en.fr', 'r', encoding = \"utf-8\")\n",
    "french = f.read()\n",
    "f.close()\n",
    "french = preprocessing(french)\n",
    "\n",
    "f = open('/content/drive/My Drive/NLP_Assignment_3/fr-en.en', 'r', encoding = \"utf-8\")\n",
    "french_english = f.read()\n",
    "f.close()\n",
    "french_english = preprocessing(french_english)\n",
    "\n",
    "\n",
    "# Reference: https://python-forum.io/Thread-Removing-punctuation-from-strings-in-lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktJxzaAptfTg"
   },
   "outputs": [],
   "source": [
    "# ----- Creating sets of unique words for each language -----\n",
    "\n",
    "\n",
    "fre_set = set([word for sentence in french for word in sentence.split()])\n",
    "\n",
    "fre_eng_set = set([word for sentence in french_english for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPdbLPoctfT6"
   },
   "source": [
    "![IBM EM Algorithm](https://slideplayer.com/slide/14108470/86/images/69/IBM+Model+1+and+EM+Algorithm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5SdVbpWtfUC"
   },
   "outputs": [],
   "source": [
    "# ----- Function to find euclidian distance -----\n",
    "# ----- Takes in two dict of dicts and checks for each combination of words the difference in probabilities -----\n",
    "# ----- Squares the difference, adds them up and return the square root of the entire term -----\n",
    "# ----- Euclidian diatance formula:  sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2 + ... + (a[n] - b[n])**2) -----\n",
    "\n",
    "\n",
    "def euclid_dist(t1, t2):\n",
    "    row_keys = t1.keys()\n",
    "    cols = list(t1.values())\n",
    "    col_keys = cols[0].keys()\n",
    "\n",
    "    result = 0\n",
    "    for (row_key, col_key) in zip(row_keys, col_keys):\n",
    "        delta = (t1[row_key][col_key] -\n",
    "                 t2[row_key][col_key]) ** 2\n",
    "        result += delta\n",
    "        \n",
    "    return result ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXlB_wuRtfTo"
   },
   "outputs": [],
   "source": [
    "# ----- Implementing IBM model 1 and EM algorithm -----\n",
    "# ----- Initializing translational probabilities t(e|f) uniformly, i.e., to a single number in-----\n",
    "# ----- t is a dict of dicts with each English word having its own dict of French words with probabilities -----\n",
    "\n",
    "\n",
    "def trans_prob(source_word_Set, target_word_set):\n",
    "    t = {word_en: {word_fr: 1/len(target_word_set)\n",
    "                  for word_fr in source_word_Set}\n",
    "        for word_en in target_word_set}\n",
    "    return t\n",
    "\n",
    "\n",
    "\n",
    "# ----- print_align_table prints the word alignment values of the language pairs -----\n",
    "\n",
    "def print_align_table(source_word_Set, target_word_set):\n",
    "    t = trans_prob(source_word_Set, target_word_set)\n",
    "    return pd.DataFrame.from_dict(t)\n",
    "\n",
    "\n",
    "\n",
    "# ----- Printing word alignment table for French-English -----\n",
    "\n",
    "print_align_table(fre_set, fre_eng_set)\n",
    "\n",
    "\n",
    "# Reference: https://stackoverflow.com/questions/33157522/create-pandas-dataframe-from-dictionary-of-dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qCa2QSTKAi4K"
   },
   "source": [
    "<br>\n",
    "\n",
    "Implementing a function that outputs the alignment for each sentence pair in the training data based on the IBM Model 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b6s0VOb0tfUJ"
   },
   "outputs": [],
   "source": [
    "# ----- Function that outputs word alignment for different language pairs -----\n",
    "\n",
    "\n",
    "def word_align(source_word_set, target_word_set, source_sentences, target_sentences):\n",
    "\n",
    "\n",
    "# ----- Initializing -----\n",
    "# Getting the uniform probabilities \n",
    "t = trans_prob(source_word_set, target_word_set)\n",
    "\n",
    "# Making a copy of t as we'll modify it later in the function \n",
    "# Deep copies take a copy of all the data structures on all levels \n",
    "prev_trans_prob = deepcopy(t)\n",
    "\n",
    "# \"converged\" is set to False and we will end the iterations once it becomes true \n",
    "# That is the value get properly converged to the right probabilities \n",
    "converged = False\n",
    "\n",
    "# Initially setting the iterations to be zero\n",
    "iterations = 0\n",
    "\n",
    "\n",
    "# while loop will loop through until values converge\n",
    "while not converged:\n",
    "\n",
    "  # Setting count(e|f) to be zero for all French words with respect to English words inside a dict of dicts \n",
    "  count = {word_en: {word_fr: 0\n",
    "                for word_fr in source_word_set}\n",
    "      for word_en in target_word_set}\n",
    "\n",
    "  # Setting all French word total to be zero inside a dict \n",
    "  total = {f:0 for f in source_word_set}\n",
    "\n",
    "\n",
    "  # Looping through all sentence pairs\n",
    "  for i in range(len(source_sentences)):  # could be len(target_senetence) as well as both have equal amount of sentences\n",
    "\n",
    "      # Looping through all English words in each English sentence\n",
    "      for e in target_sentences[i].split():  \n",
    "\n",
    "          # Initializing temp_sum to zero to later add all the word probabilities to it\n",
    "          temp_sum = 0\n",
    "\n",
    "          # Looping through all French words in each Fnglish sentence\n",
    "          for f in source_sentences[i].split():\n",
    "\n",
    "              # Translational probabilities for each english word for all french words are added together\n",
    "              temp_sum += t[e][f]\n",
    "\n",
    "          for f in source_sentences[i].split():\n",
    "\n",
    "              # Each probability is divided by the sum of the probabilies to get normalized and is added to the count of each English-French word pair\n",
    "              count[e][f] += t[e][f] / temp_sum\n",
    "\n",
    "              # Doing the same as above and storing it in the dict total \n",
    "              total[f] += t[e][f] / temp_sum\n",
    "\n",
    "\n",
    "  # Looping thro the source set\n",
    "  for f in source_word_set:\n",
    "\n",
    "      # Looping thro the target set\n",
    "      for e in target_word_set:\n",
    "\n",
    "          # Dividing count of each word pair by the 'total' variable to get the individual translational probabilited\n",
    "          t[e][f] = count[e][f] / total[f]\n",
    "\n",
    "\n",
    "  epsilon = 0.001  # threshold for convergence\n",
    "  delta = euclid_dist(prev_trans_prob, t)  # finding the distance between the previous & new probabilities\n",
    "  converged = delta < epsilon  # Converged becomes true when delta < epsilon\n",
    "  prev_trans_prob = deepcopy(t)  # Changing to the new probabilities\n",
    "  iterations += 1  # Incrementing iterations\n",
    "  print(iterations) \n",
    "\n",
    "return t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----- Calling function word_align for French-English language pairs  ----- \n",
    "\n",
    "\n",
    "t = word_align(fre_set, fre_eng_set, french, french_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NvNri8KytfUX"
   },
   "outputs": [],
   "source": [
    "# ----- Printing word alignments of French-English language pair -----\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"{0:20}{1:20}{2:20}\".format('English Words', 'French Words', 'Probabilities'))\n",
    "print(\"\")\n",
    "for word in fre_eng_set:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word, max(t[word].items(), key=operator.itemgetter(1))[0], max(t[word].items(), key=operator.itemgetter(1))[1]))\n",
    "\n",
    "\n",
    "# Reference: https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hRKYeriPcER"
   },
   "outputs": [],
   "source": [
    "# ----- Deleting data for space ----- \n",
    "\n",
    "\n",
    "del t, fre_set, fre_word_set, french, french_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLq_aacw6bEc"
   },
   "outputs": [],
   "source": [
    "# ----- Repeating the steps for the German_English language pair -----\n",
    "# ----- Reading files & preprocessing the data -----\n",
    "\n",
    "\n",
    "f = open('/content/drive/My Drive/NLP_Assignment_3/de-en.de', 'r', encoding = \"utf-8\")\n",
    "german = f.read()\n",
    "f.close()\n",
    "german = preprocessing(german)\n",
    "\n",
    "f = open('/content/drive/My Drive/NLP_Assignment_3/de-en.en', 'r', encoding = \"utf-8\")\n",
    "german_english = f.read()\n",
    "f.close()\n",
    "german_english = preprocessing(german_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WimOcjzf8kQW"
   },
   "outputs": [],
   "source": [
    "# ----- Creating sets of unique words for each language -----\n",
    "\n",
    "\n",
    "ger_set = set([word for sentence in german for word in sentence.split()])\n",
    "\n",
    "ger_eng_set = set([word for sentence in german_english for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlFYpceE9cVQ"
   },
   "outputs": [],
   "source": [
    "# ----- Printing word alignment table for German-English -----\n",
    "\n",
    "\n",
    "print_align_table(ger_set, ger_eng_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdUg2i689dER"
   },
   "outputs": [],
   "source": [
    "# ----- Calling function word_align for French-English language pairs  ----- \n",
    "\n",
    "\n",
    "t = word_align(ger_set, ger_eng_set, german, german_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cA5uJDl6PZ-W"
   },
   "outputs": [],
   "source": [
    "# ----- Printing word alignments of German-English language pair -----\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"{0:20}{1:20}{2:20}\".format('German Words', 'English Words', 'Probabilities'))\n",
    "print(\"\")\n",
    "for word in ger_eng_set:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(max(t[word].items(), key=operator.itemgetter(1))[0], word, max(t[word].items(), key=operator.itemgetter(1))[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvGwn3w9XNNW"
   },
   "source": [
    "<br>\n",
    "\n",
    "### Translating a sentence from German to English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQh0ikvtXMKG"
   },
   "outputs": [],
   "source": [
    "# ----- Making a toy data set ger_toy and eng_toy with the 2nd sentence of \"german\" and  2nd sentence of \"german_english\" -----\n",
    "\n",
    "ger_toy = german[1]\n",
    "eng_toy = german_english[1]\n",
    "\n",
    "\n",
    "# ----- Printing sentences -----\n",
    "\n",
    "print(\"German:\")\n",
    "print(ger_toy)\n",
    "print(\"Translated English:\")\n",
    "eng = []\n",
    "for word in ger_toy:\n",
    "    eng.append(max(t[word].items(), key=operator.itemgetter(1))[0])\n",
    "eng = ' '.join(eng)\n",
    "print(eng)\n",
    "print(\"Actual English\")\n",
    "print(eng_toy)\n",
    "\n",
    "\n",
    "# References: https://stackoverflow.com/questions/12453580/concatenate-item-in-list-to-strings"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Janani_Arunachalam_NLP_Assignment_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "1170px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
