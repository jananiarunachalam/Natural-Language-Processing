{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training probabilistic language models to distinguish between words in different languages - English, French, Spanish & Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----  Importing Libraries  ------\n",
    "\n",
    "\n",
    "from nltk import ngrams, FreqDist\n",
    "from nltk.corpus import udhr\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----  Retriving UDHR of different languages  ------\n",
    "\n",
    "\n",
    "english = udhr.raw('English-Latin1')\n",
    "french = udhr.raw('French_Francais-Latin1')\n",
    "italian = udhr.raw('Italian_Italiano-Latin1')\n",
    "spanish = udhr.raw('Spanish_Espanol-Latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----  Splitting into training & Dev Data  ------\n",
    "\n",
    "\n",
    "english_train, english_dev = english[0:1000], english[1000:1100]\n",
    "french_train, french_dev = french[0:1000], french[1000:1100]\n",
    "italian_train, italian_dev = italian[0:1000], italian[1000:1100]\n",
    "spanish_train, spanish_dev = spanish[0:1000], spanish[1000:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----  Getting test data  ------\n",
    "\n",
    "\n",
    "english_test = udhr.words('English-Latin1')[0:1000]\n",
    "french_test = udhr.words('French_Francais-Latin1')[0:1000]\n",
    "italian_test = udhr.words('Italian_Italiano-Latin1')[0:1000]\n",
    "spanish_test = udhr.words('Spanish_Espanol-Latin1')[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----  PREPROCESSING  ------\n",
    "\n",
    "# -----  Lowercasing training & dev data  ------\n",
    "\n",
    "\n",
    "e_train = english_train.lower()\n",
    "e_dev = english_dev.lower()\n",
    "f_train = french_train.lower()\n",
    "f_dev = french_dev.lower()\n",
    "i_train = italian_train.lower()\n",
    "i_dev = italian_dev.lower()\n",
    "s_train = spanish_train.lower()\n",
    "s_dev = spanish_dev.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----  Converting Dev data to words for easy handling by the model  ----\n",
    "\n",
    "\n",
    "e_dev = e_dev.split()\n",
    "f_dev = f_dev.split()\n",
    "i_dev = i_dev.split()\n",
    "s_dev = s_dev.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['universal',\n",
       " 'declaration',\n",
       " 'of',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'preamble',\n",
       " 'whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'rights',\n",
       " 'of',\n",
       " 'all',\n",
       " 'members',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'family',\n",
       " 'is',\n",
       " 'the',\n",
       " 'foundation',\n",
       " 'of',\n",
       " 'freedom',\n",
       " 'justice',\n",
       " 'and',\n",
       " 'peace',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'whereas',\n",
       " 'disregard',\n",
       " 'and',\n",
       " 'contempt',\n",
       " 'for',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'have',\n",
       " 'resulted',\n",
       " 'in',\n",
       " 'barbarous',\n",
       " 'acts',\n",
       " 'which',\n",
       " 'have',\n",
       " 'outraged',\n",
       " 'the',\n",
       " 'conscience',\n",
       " 'of',\n",
       " 'mankind',\n",
       " 'and',\n",
       " 'the',\n",
       " 'advent',\n",
       " 'of',\n",
       " 'a',\n",
       " 'world',\n",
       " 'in',\n",
       " 'which',\n",
       " 'human',\n",
       " 'beings',\n",
       " 'shall',\n",
       " 'enjoy',\n",
       " 'freedom',\n",
       " 'of',\n",
       " 'speech',\n",
       " 'and',\n",
       " 'belief',\n",
       " 'and',\n",
       " 'freedom',\n",
       " 'from',\n",
       " 'fear',\n",
       " 'and',\n",
       " 'want',\n",
       " 'has',\n",
       " 'been',\n",
       " 'proclaimed',\n",
       " 'as',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'aspiration',\n",
       " 'of',\n",
       " 'the',\n",
       " 'common',\n",
       " 'people',\n",
       " 'whereas',\n",
       " 'it',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'if',\n",
       " 'man',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'be',\n",
       " 'compelled',\n",
       " 'to',\n",
       " 'have',\n",
       " 'recourse',\n",
       " 'as',\n",
       " 'a',\n",
       " 'last',\n",
       " 'resort',\n",
       " 'to',\n",
       " 'rebellion',\n",
       " 'against',\n",
       " 'tyranny',\n",
       " 'and',\n",
       " 'oppression',\n",
       " 'that',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'should',\n",
       " 'be',\n",
       " 'protected',\n",
       " 'by',\n",
       " 'the',\n",
       " 'rule',\n",
       " 'of',\n",
       " 'law',\n",
       " 'whereas',\n",
       " 'it',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'to',\n",
       " 'promote',\n",
       " 'the',\n",
       " 'development',\n",
       " 'of',\n",
       " 'friendly',\n",
       " 'relations',\n",
       " 'between',\n",
       " 'nations',\n",
       " 'whereas',\n",
       " 'the',\n",
       " 'peoples',\n",
       " 'of',\n",
       " 'the',\n",
       " 'united',\n",
       " 'nations',\n",
       " 'have',\n",
       " 'in',\n",
       " 'the',\n",
       " 'charter',\n",
       " 'reaffirmed',\n",
       " 'their',\n",
       " 'faith',\n",
       " 'in',\n",
       " 'fundamental',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'in',\n",
       " 'the',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'person',\n",
       " 'and',\n",
       " 'in']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----  Tokenizing training data  ------\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\n",
    "\n",
    "english_train_tokenized = tokenizer.tokenize(e_train)\n",
    "french_train_tokenized = tokenizer.tokenize(f_train)\n",
    "italian_train_tokenized = tokenizer.tokenize(i_train)\n",
    "spanish_train_tokenized = tokenizer.tokenize(s_train)\n",
    "\n",
    "english_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__u',\n",
       " '_un',\n",
       " 'uni',\n",
       " 'niv',\n",
       " 'ive',\n",
       " 'ver',\n",
       " 'ers',\n",
       " 'rsa',\n",
       " 'sal',\n",
       " 'al_',\n",
       " 'l__',\n",
       " '__d',\n",
       " '_de',\n",
       " 'dec',\n",
       " 'ecl',\n",
       " 'cla',\n",
       " 'lar',\n",
       " 'ara',\n",
       " 'rat',\n",
       " 'ati',\n",
       " 'tio',\n",
       " 'ion',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__h',\n",
       " '_hu',\n",
       " 'hum',\n",
       " 'uma',\n",
       " 'man',\n",
       " 'an_',\n",
       " 'n__',\n",
       " '__r',\n",
       " '_ri',\n",
       " 'rig',\n",
       " 'igh',\n",
       " 'ght',\n",
       " 'hts',\n",
       " 'ts_',\n",
       " 's__',\n",
       " '__p',\n",
       " '_pr',\n",
       " 'pre',\n",
       " 'rea',\n",
       " 'eam',\n",
       " 'amb',\n",
       " 'mbl',\n",
       " 'ble',\n",
       " 'le_',\n",
       " 'e__',\n",
       " '__w',\n",
       " '_wh',\n",
       " 'whe',\n",
       " 'her',\n",
       " 'ere',\n",
       " 'rea',\n",
       " 'eas',\n",
       " 'as_',\n",
       " 's__',\n",
       " '__r',\n",
       " '_re',\n",
       " 'rec',\n",
       " 'eco',\n",
       " 'cog',\n",
       " 'ogn',\n",
       " 'gni',\n",
       " 'nit',\n",
       " 'iti',\n",
       " 'tio',\n",
       " 'ion',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__i',\n",
       " '_in',\n",
       " 'inh',\n",
       " 'nhe',\n",
       " 'her',\n",
       " 'ere',\n",
       " 'ren',\n",
       " 'ent',\n",
       " 'nt_',\n",
       " 't__',\n",
       " '__d',\n",
       " '_di',\n",
       " 'dig',\n",
       " 'ign',\n",
       " 'gni',\n",
       " 'nit',\n",
       " 'ity',\n",
       " 'ty_',\n",
       " 'y__',\n",
       " '__a',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__e',\n",
       " '_eq',\n",
       " 'equ',\n",
       " 'qua',\n",
       " 'ual',\n",
       " 'al_',\n",
       " 'l__',\n",
       " '__a',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__i',\n",
       " '_in',\n",
       " 'ina',\n",
       " 'nal',\n",
       " 'ali',\n",
       " 'lie',\n",
       " 'ien',\n",
       " 'ena',\n",
       " 'nab',\n",
       " 'abl',\n",
       " 'ble',\n",
       " 'le_',\n",
       " 'e__',\n",
       " '__r',\n",
       " '_ri',\n",
       " 'rig',\n",
       " 'igh',\n",
       " 'ght',\n",
       " 'hts',\n",
       " 'ts_',\n",
       " 's__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__a',\n",
       " '_al',\n",
       " 'all',\n",
       " 'll_',\n",
       " 'l__',\n",
       " '__m',\n",
       " '_me',\n",
       " 'mem',\n",
       " 'emb',\n",
       " 'mbe',\n",
       " 'ber',\n",
       " 'ers',\n",
       " 'rs_',\n",
       " 's__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__h',\n",
       " '_hu',\n",
       " 'hum',\n",
       " 'uma',\n",
       " 'man',\n",
       " 'an_',\n",
       " 'n__',\n",
       " '__f',\n",
       " '_fa',\n",
       " 'fam',\n",
       " 'ami',\n",
       " 'mil',\n",
       " 'ily',\n",
       " 'ly_',\n",
       " 'y__',\n",
       " '__i',\n",
       " '_is',\n",
       " 'is_',\n",
       " 's__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__f',\n",
       " '_fo',\n",
       " 'fou',\n",
       " 'oun',\n",
       " 'und',\n",
       " 'nda',\n",
       " 'dat',\n",
       " 'ati',\n",
       " 'tio',\n",
       " 'ion',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__f',\n",
       " '_fr',\n",
       " 'fre',\n",
       " 'ree',\n",
       " 'eed',\n",
       " 'edo',\n",
       " 'dom',\n",
       " 'om_',\n",
       " 'm__',\n",
       " '__j',\n",
       " '_ju',\n",
       " 'jus',\n",
       " 'ust',\n",
       " 'sti',\n",
       " 'tic',\n",
       " 'ice',\n",
       " 'ce_',\n",
       " 'e__',\n",
       " '__a',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__p',\n",
       " '_pe',\n",
       " 'pea',\n",
       " 'eac',\n",
       " 'ace',\n",
       " 'ce_',\n",
       " 'e__',\n",
       " '__i',\n",
       " '_in',\n",
       " 'in_',\n",
       " 'n__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__w',\n",
       " '_wo',\n",
       " 'wor',\n",
       " 'orl',\n",
       " 'rld',\n",
       " 'ld_',\n",
       " 'd__',\n",
       " '__w',\n",
       " '_wh',\n",
       " 'whe',\n",
       " 'her',\n",
       " 'ere',\n",
       " 'rea',\n",
       " 'eas',\n",
       " 'as_',\n",
       " 's__',\n",
       " '__d',\n",
       " '_di',\n",
       " 'dis',\n",
       " 'isr',\n",
       " 'sre',\n",
       " 'reg',\n",
       " 'ega',\n",
       " 'gar',\n",
       " 'ard',\n",
       " 'rd_',\n",
       " 'd__',\n",
       " '__a',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__c',\n",
       " '_co',\n",
       " 'con',\n",
       " 'ont',\n",
       " 'nte',\n",
       " 'tem',\n",
       " 'emp',\n",
       " 'mpt',\n",
       " 'pt_',\n",
       " 't__',\n",
       " '__f',\n",
       " '_fo',\n",
       " 'for',\n",
       " 'or_',\n",
       " 'r__',\n",
       " '__h',\n",
       " '_hu',\n",
       " 'hum',\n",
       " 'uma',\n",
       " 'man',\n",
       " 'an_',\n",
       " 'n__',\n",
       " '__r',\n",
       " '_ri',\n",
       " 'rig',\n",
       " 'igh',\n",
       " 'ght',\n",
       " 'hts',\n",
       " 'ts_',\n",
       " 's__',\n",
       " '__h',\n",
       " '_ha',\n",
       " 'hav',\n",
       " 'ave',\n",
       " 've_',\n",
       " 'e__',\n",
       " '__r',\n",
       " '_re',\n",
       " 'res',\n",
       " 'esu',\n",
       " 'sul',\n",
       " 'ult',\n",
       " 'lte',\n",
       " 'ted',\n",
       " 'ed_',\n",
       " 'd__',\n",
       " '__i',\n",
       " '_in',\n",
       " 'in_',\n",
       " 'n__',\n",
       " '__b',\n",
       " '_ba',\n",
       " 'bar',\n",
       " 'arb',\n",
       " 'rba',\n",
       " 'bar',\n",
       " 'aro',\n",
       " 'rou',\n",
       " 'ous',\n",
       " 'us_',\n",
       " 's__',\n",
       " '__a',\n",
       " '_ac',\n",
       " 'act',\n",
       " 'cts',\n",
       " 'ts_',\n",
       " 's__',\n",
       " '__w',\n",
       " '_wh',\n",
       " 'whi',\n",
       " 'hic',\n",
       " 'ich',\n",
       " 'ch_',\n",
       " 'h__',\n",
       " '__h',\n",
       " '_ha',\n",
       " 'hav',\n",
       " 'ave',\n",
       " 've_',\n",
       " 'e__',\n",
       " '__o',\n",
       " '_ou',\n",
       " 'out',\n",
       " 'utr',\n",
       " 'tra',\n",
       " 'rag',\n",
       " 'age',\n",
       " 'ged',\n",
       " 'ed_',\n",
       " 'd__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__c',\n",
       " '_co',\n",
       " 'con',\n",
       " 'ons',\n",
       " 'nsc',\n",
       " 'sci',\n",
       " 'cie',\n",
       " 'ien',\n",
       " 'enc',\n",
       " 'nce',\n",
       " 'ce_',\n",
       " 'e__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__m',\n",
       " '_ma',\n",
       " 'man',\n",
       " 'ank',\n",
       " 'nki',\n",
       " 'kin',\n",
       " 'ind',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__a',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__a',\n",
       " '_ad',\n",
       " 'adv',\n",
       " 'dve',\n",
       " 'ven',\n",
       " 'ent',\n",
       " 'nt_',\n",
       " 't__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__a',\n",
       " '_a_',\n",
       " 'a__',\n",
       " '__w',\n",
       " '_wo',\n",
       " 'wor',\n",
       " 'orl',\n",
       " 'rld',\n",
       " 'ld_',\n",
       " 'd__',\n",
       " '__i',\n",
       " '_in',\n",
       " 'in_',\n",
       " 'n__',\n",
       " '__w',\n",
       " '_wh',\n",
       " 'whi',\n",
       " 'hic',\n",
       " 'ich',\n",
       " 'ch_',\n",
       " 'h__',\n",
       " '__h',\n",
       " '_hu',\n",
       " 'hum',\n",
       " 'uma',\n",
       " 'man',\n",
       " 'an_',\n",
       " 'n__',\n",
       " '__b',\n",
       " '_be',\n",
       " 'bei',\n",
       " 'ein',\n",
       " 'ing',\n",
       " 'ngs',\n",
       " 'gs_',\n",
       " 's__',\n",
       " '__s',\n",
       " '_sh',\n",
       " 'sha',\n",
       " 'hal',\n",
       " 'all',\n",
       " 'll_',\n",
       " 'l__',\n",
       " '__e',\n",
       " '_en',\n",
       " 'enj',\n",
       " 'njo',\n",
       " 'joy',\n",
       " 'oy_',\n",
       " 'y__',\n",
       " '__f',\n",
       " '_fr',\n",
       " 'fre',\n",
       " 'ree',\n",
       " 'eed',\n",
       " 'edo',\n",
       " 'dom',\n",
       " 'om_',\n",
       " 'm__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__s',\n",
       " '_sp',\n",
       " 'spe',\n",
       " 'pee',\n",
       " 'eec',\n",
       " 'ech',\n",
       " 'ch_',\n",
       " 'h__',\n",
       " '__a',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__b',\n",
       " '_be',\n",
       " 'bel',\n",
       " 'eli',\n",
       " 'lie',\n",
       " 'ief',\n",
       " 'ef_',\n",
       " 'f__',\n",
       " '__a',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__f',\n",
       " '_fr',\n",
       " 'fre',\n",
       " 'ree',\n",
       " 'eed',\n",
       " 'edo',\n",
       " 'dom',\n",
       " 'om_',\n",
       " 'm__',\n",
       " '__f',\n",
       " '_fr',\n",
       " 'fro',\n",
       " 'rom',\n",
       " 'om_',\n",
       " 'm__',\n",
       " '__f',\n",
       " '_fe',\n",
       " 'fea',\n",
       " 'ear',\n",
       " 'ar_',\n",
       " 'r__',\n",
       " '__a',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__w',\n",
       " '_wa',\n",
       " 'wan',\n",
       " 'ant',\n",
       " 'nt_',\n",
       " 't__',\n",
       " '__h',\n",
       " '_ha',\n",
       " 'has',\n",
       " 'as_',\n",
       " 's__',\n",
       " '__b',\n",
       " '_be',\n",
       " 'bee',\n",
       " 'een',\n",
       " 'en_',\n",
       " 'n__',\n",
       " '__p',\n",
       " '_pr',\n",
       " 'pro',\n",
       " 'roc',\n",
       " 'ocl',\n",
       " 'cla',\n",
       " 'lai',\n",
       " 'aim',\n",
       " 'ime',\n",
       " 'med',\n",
       " 'ed_',\n",
       " 'd__',\n",
       " '__a',\n",
       " '_as',\n",
       " 'as_',\n",
       " 's__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__h',\n",
       " '_hi',\n",
       " 'hig',\n",
       " 'igh',\n",
       " 'ghe',\n",
       " 'hes',\n",
       " 'est',\n",
       " 'st_',\n",
       " 't__',\n",
       " '__a',\n",
       " '_as',\n",
       " 'asp',\n",
       " 'spi',\n",
       " 'pir',\n",
       " 'ira',\n",
       " 'rat',\n",
       " 'ati',\n",
       " 'tio',\n",
       " 'ion',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__c',\n",
       " '_co',\n",
       " 'com',\n",
       " 'omm',\n",
       " 'mmo',\n",
       " 'mon',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__p',\n",
       " '_pe',\n",
       " 'peo',\n",
       " 'eop',\n",
       " 'opl',\n",
       " 'ple',\n",
       " 'le_',\n",
       " 'e__',\n",
       " '__w',\n",
       " '_wh',\n",
       " 'whe',\n",
       " 'her',\n",
       " 'ere',\n",
       " 'rea',\n",
       " 'eas',\n",
       " 'as_',\n",
       " 's__',\n",
       " '__i',\n",
       " '_it',\n",
       " 'it_',\n",
       " 't__',\n",
       " '__i',\n",
       " '_is',\n",
       " 'is_',\n",
       " 's__',\n",
       " '__e',\n",
       " '_es',\n",
       " 'ess',\n",
       " 'sse',\n",
       " 'sen',\n",
       " 'ent',\n",
       " 'nti',\n",
       " 'tia',\n",
       " 'ial',\n",
       " 'al_',\n",
       " 'l__',\n",
       " '__i',\n",
       " '_if',\n",
       " 'if_',\n",
       " 'f__',\n",
       " '__m',\n",
       " '_ma',\n",
       " 'man',\n",
       " 'an_',\n",
       " 'n__',\n",
       " '__i',\n",
       " '_is',\n",
       " 'is_',\n",
       " 's__',\n",
       " '__n',\n",
       " '_no',\n",
       " 'not',\n",
       " 'ot_',\n",
       " 't__',\n",
       " '__t',\n",
       " '_to',\n",
       " 'to_',\n",
       " 'o__',\n",
       " '__b',\n",
       " '_be',\n",
       " 'be_',\n",
       " 'e__',\n",
       " '__c',\n",
       " '_co',\n",
       " 'com',\n",
       " 'omp',\n",
       " 'mpe',\n",
       " 'pel',\n",
       " 'ell',\n",
       " 'lle',\n",
       " 'led',\n",
       " 'ed_',\n",
       " 'd__',\n",
       " '__t',\n",
       " '_to',\n",
       " 'to_',\n",
       " 'o__',\n",
       " '__h',\n",
       " '_ha',\n",
       " 'hav',\n",
       " 'ave',\n",
       " 've_',\n",
       " 'e__',\n",
       " '__r',\n",
       " '_re',\n",
       " 'rec',\n",
       " 'eco',\n",
       " 'cou',\n",
       " 'our',\n",
       " 'urs',\n",
       " 'rse',\n",
       " 'se_',\n",
       " 'e__',\n",
       " '__a',\n",
       " '_as',\n",
       " 'as_',\n",
       " 's__',\n",
       " '__a',\n",
       " '_a_',\n",
       " 'a__',\n",
       " '__l',\n",
       " '_la',\n",
       " 'las',\n",
       " 'ast',\n",
       " 'st_',\n",
       " 't__',\n",
       " '__r',\n",
       " '_re',\n",
       " 'res',\n",
       " 'eso',\n",
       " 'sor',\n",
       " 'ort',\n",
       " 'rt_',\n",
       " 't__',\n",
       " '__t',\n",
       " '_to',\n",
       " 'to_',\n",
       " 'o__',\n",
       " '__r',\n",
       " '_re',\n",
       " 'reb',\n",
       " 'ebe',\n",
       " 'bel',\n",
       " 'ell',\n",
       " 'lli',\n",
       " 'lio',\n",
       " 'ion',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__a',\n",
       " '_ag',\n",
       " 'aga',\n",
       " 'gai',\n",
       " 'ain',\n",
       " 'ins',\n",
       " 'nst',\n",
       " 'st_',\n",
       " 't__',\n",
       " '__t',\n",
       " '_ty',\n",
       " 'tyr',\n",
       " 'yra',\n",
       " 'ran',\n",
       " 'ann',\n",
       " 'nny',\n",
       " 'ny_',\n",
       " 'y__',\n",
       " '__a',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__o',\n",
       " '_op',\n",
       " 'opp',\n",
       " 'ppr',\n",
       " 'pre',\n",
       " 'res',\n",
       " 'ess',\n",
       " 'ssi',\n",
       " 'sio',\n",
       " 'ion',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'tha',\n",
       " 'hat',\n",
       " 'at_',\n",
       " 't__',\n",
       " '__h',\n",
       " '_hu',\n",
       " 'hum',\n",
       " 'uma',\n",
       " 'man',\n",
       " 'an_',\n",
       " 'n__',\n",
       " '__r',\n",
       " '_ri',\n",
       " 'rig',\n",
       " 'igh',\n",
       " 'ght',\n",
       " 'hts',\n",
       " 'ts_',\n",
       " 's__',\n",
       " '__s',\n",
       " '_sh',\n",
       " 'sho',\n",
       " 'hou',\n",
       " 'oul',\n",
       " 'uld',\n",
       " 'ld_',\n",
       " 'd__',\n",
       " '__b',\n",
       " '_be',\n",
       " 'be_',\n",
       " 'e__',\n",
       " '__p',\n",
       " '_pr',\n",
       " 'pro',\n",
       " 'rot',\n",
       " 'ote',\n",
       " 'tec',\n",
       " 'ect',\n",
       " 'cte',\n",
       " 'ted',\n",
       " 'ed_',\n",
       " 'd__',\n",
       " '__b',\n",
       " '_by',\n",
       " 'by_',\n",
       " 'y__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__r',\n",
       " '_ru',\n",
       " 'rul',\n",
       " 'ule',\n",
       " 'le_',\n",
       " 'e__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__l',\n",
       " '_la',\n",
       " 'law',\n",
       " 'aw_',\n",
       " 'w__',\n",
       " '__w',\n",
       " '_wh',\n",
       " 'whe',\n",
       " 'her',\n",
       " 'ere',\n",
       " 'rea',\n",
       " 'eas',\n",
       " 'as_',\n",
       " 's__',\n",
       " '__i',\n",
       " '_it',\n",
       " 'it_',\n",
       " 't__',\n",
       " '__i',\n",
       " '_is',\n",
       " 'is_',\n",
       " 's__',\n",
       " '__e',\n",
       " '_es',\n",
       " 'ess',\n",
       " 'sse',\n",
       " 'sen',\n",
       " 'ent',\n",
       " 'nti',\n",
       " 'tia',\n",
       " 'ial',\n",
       " 'al_',\n",
       " 'l__',\n",
       " '__t',\n",
       " '_to',\n",
       " 'to_',\n",
       " 'o__',\n",
       " '__p',\n",
       " '_pr',\n",
       " 'pro',\n",
       " 'rom',\n",
       " 'omo',\n",
       " 'mot',\n",
       " 'ote',\n",
       " 'te_',\n",
       " 'e__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__d',\n",
       " '_de',\n",
       " 'dev',\n",
       " 'eve',\n",
       " 'vel',\n",
       " 'elo',\n",
       " 'lop',\n",
       " 'opm',\n",
       " 'pme',\n",
       " 'men',\n",
       " 'ent',\n",
       " 'nt_',\n",
       " 't__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__f',\n",
       " '_fr',\n",
       " 'fri',\n",
       " 'rie',\n",
       " 'ien',\n",
       " 'end',\n",
       " 'ndl',\n",
       " 'dly',\n",
       " 'ly_',\n",
       " 'y__',\n",
       " '__r',\n",
       " '_re',\n",
       " 'rel',\n",
       " 'ela',\n",
       " 'lat',\n",
       " 'ati',\n",
       " 'tio',\n",
       " 'ion',\n",
       " 'ons',\n",
       " 'ns_',\n",
       " 's__',\n",
       " '__b',\n",
       " '_be',\n",
       " 'bet',\n",
       " 'etw',\n",
       " 'twe',\n",
       " 'wee',\n",
       " 'een',\n",
       " 'en_',\n",
       " 'n__',\n",
       " '__n',\n",
       " '_na',\n",
       " 'nat',\n",
       " 'ati',\n",
       " 'tio',\n",
       " 'ion',\n",
       " 'ons',\n",
       " 'ns_',\n",
       " 's__',\n",
       " '__w',\n",
       " '_wh',\n",
       " 'whe',\n",
       " 'her',\n",
       " 'ere',\n",
       " 'rea',\n",
       " 'eas',\n",
       " 'as_',\n",
       " 's__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__p',\n",
       " '_pe',\n",
       " 'peo',\n",
       " 'eop',\n",
       " 'opl',\n",
       " 'ple',\n",
       " 'les',\n",
       " 'es_',\n",
       " 's__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__u',\n",
       " '_un',\n",
       " 'uni',\n",
       " 'nit',\n",
       " 'ite',\n",
       " 'ted',\n",
       " 'ed_',\n",
       " 'd__',\n",
       " '__n',\n",
       " '_na',\n",
       " 'nat',\n",
       " 'ati',\n",
       " 'tio',\n",
       " ...]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----  N-grams creation  -----\n",
    "\n",
    "\n",
    "# n gram creator function\n",
    "def ngramer(data, num):\n",
    "    temp = []\n",
    "    for word in data:\n",
    "        temp.append(list(ngrams(word, num, pad_left=True, pad_right=True, left_pad_symbol=\"_\", right_pad_symbol=\"_\")))\n",
    "    temp = [word for sublist in temp for word in sublist]\n",
    "    n_grams = temp\n",
    "    for i, v in enumerate(temp):\n",
    "        n_grams[i] = ''.join(v)\n",
    "    return temp\n",
    "\n",
    "\n",
    "\n",
    "eng_train_unigrams = ngramer(english_train_tokenized, 1)\n",
    "eng_train_bigrams = ngramer(english_train_tokenized, 2)\n",
    "eng_train_trigrams = ngramer(english_train_tokenized, 3)\n",
    "\n",
    "fre_train_unigrams = ngramer(french_train_tokenized, 1)\n",
    "fre_train_bigrams = ngramer(french_train_tokenized, 2)\n",
    "fre_train_trigrams = ngramer(french_train_tokenized, 3)\n",
    "\n",
    "ita_train_unigrams = ngramer(italian_train_tokenized, 1)\n",
    "ita_train_bigrams = ngramer(italian_train_tokenized, 2)\n",
    "ita_train_trigrams = ngramer(italian_train_tokenized, 3)\n",
    "\n",
    "spa_train_unigrams = ngramer(spanish_train_tokenized, 1)\n",
    "spa_train_bigrams = ngramer(spanish_train_tokenized, 2)\n",
    "spa_train_trigrams = ngramer(spanish_train_tokenized, 3)\n",
    "\n",
    "eng_train_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'e__': 31, 'n__': 25, 's__': 25, '__t': 23, 'd__': 23, '__a': 20, '_th': 18, 'the': 17, '__o': 16, 'f__': 16, ...})"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----  Freq. Distribution of N-grams  -----\n",
    "\n",
    "\n",
    "eng_unigrams_freqdist = FreqDist(eng_train_unigrams)\n",
    "eng_bigrams_freqdist = FreqDist(eng_train_bigrams)\n",
    "eng_trigrams_freqdist = FreqDist(eng_train_trigrams)\n",
    "\n",
    "fre_unigrams_freqdist = FreqDist(fre_train_unigrams)\n",
    "fre_bigrams_freqdist = FreqDist(fre_train_bigrams)\n",
    "fre_trigrams_freqdist = FreqDist(fre_train_trigrams)\n",
    "\n",
    "ita_unigrams_freqdist = FreqDist(ita_train_unigrams)\n",
    "ita_bigrams_freqdist = FreqDist(ita_train_bigrams)\n",
    "ita_trigrams_freqdist = FreqDist(ita_train_trigrams)\n",
    "\n",
    "spa_unigrams_freqdist = FreqDist(spa_train_unigrams)\n",
    "spa_bigrams_freqdist = FreqDist(spa_train_bigrams)\n",
    "spa_trigrams_freqdist = FreqDist(spa_train_trigrams)\n",
    "\n",
    "eng_trigrams_freqdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Test data goes into a function called \"uni_english_french\" as an argument. Pre-processing is done by lowering the text and nothing else is done as the text is already seperate words. Test data goes in with frequency distribution of english training words along with the length of the english training unigrams data. Same happens with the again but now with the frequency distribution of french training words and the length of the french training unigrams data. Inside the uni_lang_prob function the probability of the all the test data words of returned in as a list. The english word probability and french word probability lists are compared for every word and whichever language probability is the highest is given a score of 1. Thus keeping track of the score will give us the most probablity of the language the test data belongs to.\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Model accuracy of English is 70.58823529411765%.\n",
      "Unigram Model accuracy of French is 29.411764705882355%.\n"
     ]
    }
   ],
   "source": [
    "# English v. French Unigram Model with English test data  ----- \n",
    "\n",
    "\n",
    "def uni_lang_prob(test_data, freqdist, total):\n",
    "    lang_prob = []\n",
    "    for word in test_data:\n",
    "        word_prob = 1\n",
    "        for char in word:\n",
    "            word_prob *= freqdist[char]/total\n",
    "        lang_prob.append(word_prob)\n",
    "    return lang_prob\n",
    "\n",
    "\n",
    "def uni_english_french(test_data):\n",
    "    len_test_data = len(test_data)\n",
    "    test_data = [w.lower() for w in test_data]\n",
    "    temp_eng_accuracy = temp_fre_accuracy = 0\n",
    "    eng_prob = uni_lang_prob(test_data, eng_unigrams_freqdist, len(eng_train_unigrams))\n",
    "    fre_prob = uni_lang_prob(test_data, fre_unigrams_freqdist, len(fre_train_unigrams))\n",
    "    for i in range(len_test_data):\n",
    "        if eng_prob[i] >= fre_prob[i]:\n",
    "            temp_eng_accuracy += 1\n",
    "        else:\n",
    "            temp_fre_accuracy += 1\n",
    "    uni_eng_accuracy = (temp_eng_accuracy/len_test_data)*100\n",
    "    uni_fre_accuracy = (temp_fre_accuracy/len_test_data)*100\n",
    "    print(f\"Unigram Model accuracy of English is {uni_eng_accuracy}%.\")\n",
    "    print(f\"Unigram Model accuracy of French is {uni_fre_accuracy}%.\")\n",
    "\n",
    "    \n",
    "# ----- DEV DATA -----\n",
    "\n",
    "uni_english_french(e_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Model accuracy of English is 78.4%.\n",
      "Unigram Model accuracy of French is 21.6%.\n"
     ]
    }
   ],
   "source": [
    "# ----- TEST DATA -----\n",
    "\n",
    "\n",
    "uni_english_french(english_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "Almost the same procedure given for the unigram model is followed for bigram model as well except for inside the bi_lang_prob function the word probability is calculated by doing count(bigram)+1/count(unigram)+count(vocabulary), the formula for which was taken from the 3rd chapter in the Speech and Patter Recognition - https://web.stanford.edu/~jurafsky/slp3/3.pdf.\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy of English Bigrams is 88.23529411764706%.\n",
      "Model accuracy of French Bigrams is 11.76470588235294%.\n"
     ]
    }
   ],
   "source": [
    "# English v. French Bigram model with English test data  -----\n",
    "\n",
    "\n",
    "def bi_lang_prob(test_data, unigrams_freqdist, bigrams_freqdist, v):\n",
    "    lang_prob = []\n",
    "    for word in test_data:\n",
    "        word_prob = 1\n",
    "        word = \"_\"+word+\"_\"\n",
    "        for char in range(len(word)-1):\n",
    "            word_prob *= ((bigrams_freqdist[word[char:char+2]])+1)/(unigrams_freqdist[word[char]]+v)\n",
    "        lang_prob.append(word_prob)\n",
    "    return lang_prob\n",
    "    \n",
    "    \n",
    "def bi_english_french(test_data):\n",
    "    len_test_data = len(test_data)\n",
    "    test_data = [w.lower() for w in test_data]\n",
    "    temp_eng_accuracy = temp_fre_accuracy = 0\n",
    "    eng_prob = bi_lang_prob(test_data, eng_unigrams_freqdist, eng_bigrams_freqdist, len(set(eng_train_unigrams)))\n",
    "    fre_prob = bi_lang_prob(test_data, fre_unigrams_freqdist, fre_bigrams_freqdist, len(set(fre_train_unigrams)))\n",
    "    for i in range(len(eng_prob)):\n",
    "        if eng_prob[i] >= fre_prob[i]:\n",
    "            temp_eng_accuracy += 1\n",
    "        else:\n",
    "            temp_fre_accuracy += 1\n",
    "    bi_eng_accuracy = (temp_eng_accuracy/len(eng_prob))*100\n",
    "    bi_fre_accuracy = (temp_fre_accuracy/len(eng_prob))*100\n",
    "    print(f\"Model accuracy of English Bigrams is {bi_eng_accuracy}%.\")\n",
    "    print(f\"Model accuracy of French Bigrams is {bi_fre_accuracy}%.\")\n",
    "\n",
    "\n",
    "# ----- DEV DATA -----\n",
    "\n",
    "bi_english_french(e_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy of English Bigrams is 87.4%.\n",
      "Model accuracy of French Bigrams is 12.6%.\n"
     ]
    }
   ],
   "source": [
    "# ----- TEST DATA -----\n",
    "\n",
    "\n",
    "bi_english_french(english_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Almost the same procedure given for the unigram model is followed for trigram as well except for inside the tri_lang_prob function the word probability is calculated by doing count(trigram)+1/count(bigram)+count(vocabulary), the formula for which was taken from the 3rd chapter in the Speech and Patter Recognition - https://web.stanford.edu/~jurafsky/slp3/3.pdf.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy of English Trigrams is 100.0%.\n",
      "Model accuracy of French Trigrams is 0.0%.\n"
     ]
    }
   ],
   "source": [
    "# ----- English v. French Trigram model with English test data  -----\n",
    "\n",
    "\n",
    "def tri_lang_prob(test_data, bigrams_freqdist, trigrams_freqdist, v):\n",
    "    lang_prob = []\n",
    "    for word in test_data:\n",
    "        word_prob = 1\n",
    "        word = \"__\"+word+\"__\"\n",
    "        for char in range(len(word)-2):\n",
    "            word_prob *= ((trigrams_freqdist[word[char:char+3]]+1)/(bigrams_freqdist[word[char:char+2]]+v))\n",
    "        lang_prob.append(word_prob)\n",
    "    return lang_prob\n",
    "    \n",
    "    \n",
    "def tri_english_french(test_data):\n",
    "    len_test_data = len(test_data)\n",
    "    test_data = [w.lower() for w in test_data]\n",
    "    temp_eng_accuracy = temp_fre_accuracy = 0\n",
    "    eng_prob = tri_lang_prob(test_data, eng_bigrams_freqdist, eng_trigrams_freqdist, len(set(eng_train_unigrams)))\n",
    "    fre_prob = tri_lang_prob(test_data, fre_bigrams_freqdist, fre_trigrams_freqdist, len(set(fre_train_unigrams)))\n",
    "    for i in range(len(eng_prob)):\n",
    "        if eng_prob[i] >= fre_prob[i]:\n",
    "            temp_eng_accuracy += 1\n",
    "        else:\n",
    "            temp_fre_accuracy += 1\n",
    "    tri_eng_accuracy = (temp_eng_accuracy/len(eng_prob))*100\n",
    "    tri_fre_accuracy = (temp_fre_accuracy/len(eng_prob))*100\n",
    "    print(f\"Model accuracy of English Trigrams is {tri_eng_accuracy}%.\")\n",
    "    print(f\"Model accuracy of French Trigrams is {tri_fre_accuracy}%.\")\n",
    "\n",
    "    \n",
    "# ----- DEV DATA -----\n",
    "\n",
    "tri_english_french(e_dev)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy of English Trigrams is 90.10000000000001%.\n",
      "Model accuracy of French Trigrams is 9.9%.\n"
     ]
    }
   ],
   "source": [
    "# ----- TEST DATA -----\n",
    "\n",
    "tri_english_french(english_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The uni_lang_prob, bi_lang_prob, tri_lang_prob functions were reused for spanish v italian.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy of Spanish is 27.900000000000002%.\n",
      "Model accuracy of Italian is 59.199999999999996%.\n"
     ]
    }
   ],
   "source": [
    "# -----  Spanish v. English Unigram Model with Italian test data  -----\n",
    "\n",
    "\n",
    "\n",
    "def uni_spanish_italian(test_data):\n",
    "    len_test_data = len(test_data)\n",
    "    test_data = [w.lower() for w in test_data]\n",
    "    temp_spa_accuracy = temp_ita_accuracy = 0\n",
    "    spa_prob = uni_lang_prob(test_data, spa_unigrams_freqdist, len(spa_train_unigrams))\n",
    "    ita_prob = uni_lang_prob(test_data, ita_unigrams_freqdist, len(spa_train_unigrams))\n",
    "    for i in range(len_test_data):\n",
    "        if spa_prob[i] > ita_prob[i]:\n",
    "            temp_spa_accuracy += 1\n",
    "        elif spa_prob[i] < ita_prob[i]:\n",
    "            temp_ita_accuracy += 1\n",
    "        else:\n",
    "            continue\n",
    "    uni_spa_accuracy = (temp_spa_accuracy/len_test_data)*100\n",
    "    uni_ita_accuracy = (temp_ita_accuracy/len_test_data)*100\n",
    "    print(f\"Model accuracy of Spanish is {uni_spa_accuracy}%.\")\n",
    "    print(f\"Model accuracy of Italian is {uni_ita_accuracy}%.\")\n",
    "    \n",
    "\n",
    "uni_spanish_italian(italian_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy of Spanish Bigrams is 76.0%.\n",
      "Model accuracy of Italian Bigrams is 13.5%.\n"
     ]
    }
   ],
   "source": [
    "# -----  Spanish v. Italian Bigram model with Spanish test data  -----\n",
    "\n",
    "\n",
    "\n",
    "def bi_spanish_italian(test_data):\n",
    "    len_test_data = len(test_data)\n",
    "    test_data = [w.lower() for w in test_data]\n",
    "    temp_spa_accuracy = temp_ita_accuracy = 0\n",
    "    spa_prob = bi_lang_prob(test_data, spa_unigrams_freqdist, spa_bigrams_freqdist, len(set(spa_train_unigrams)))\n",
    "    ita_prob = bi_lang_prob(test_data, ita_unigrams_freqdist, ita_bigrams_freqdist, len(set(spa_train_unigrams)))\n",
    "    for i in range(len(spa_prob)):\n",
    "        if spa_prob[i] > ita_prob[i]:\n",
    "            temp_spa_accuracy += 1\n",
    "        elif spa_prob[i] < ita_prob[i]:\n",
    "            temp_ita_accuracy += 1\n",
    "        else:\n",
    "            continue\n",
    "    bi_spa_accuracy = (temp_spa_accuracy/len_test_data)*100\n",
    "    bi_ita_accuracy = (temp_ita_accuracy/len_test_data)*100\n",
    "    print(f\"Model accuracy of Spanish Bigrams is {bi_spa_accuracy}%.\")\n",
    "    print(f\"Model accuracy of Italian Bigrams is {bi_ita_accuracy}%.\")\n",
    "\n",
    "    \n",
    "bi_spanish_italian(spanish_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy of Spanish Trigrams is 22.1%.\n",
      "Model accuracy of Italian Trigrams is 66.60000000000001%.\n"
     ]
    }
   ],
   "source": [
    "# -----  Spanish v. Italian Trigram model with Italian test data  -----\n",
    "\n",
    "\n",
    "\n",
    "def tri_spanish_italian(test_data):\n",
    "    len_test_data = len(test_data)\n",
    "    test_data = [w.lower() for w in test_data]\n",
    "    temp_spa_accuracy = temp_ita_accuracy = 0\n",
    "    spa_prob = tri_lang_prob(test_data, spa_bigrams_freqdist, spa_trigrams_freqdist, len(set(spa_train_unigrams)))\n",
    "    ita_prob = tri_lang_prob(test_data, ita_bigrams_freqdist, ita_trigrams_freqdist, len(set(spa_train_unigrams)))\n",
    "    for i in range(len_test_data):\n",
    "        if spa_prob[i] > ita_prob[i]:\n",
    "            temp_spa_accuracy += 1\n",
    "        elif spa_prob[i] < ita_prob[i]:\n",
    "            temp_ita_accuracy += 1\n",
    "        else:\n",
    "            continue\n",
    "    tri_spa_accuracy = (temp_spa_accuracy/len_test_data)*100\n",
    "    tri_ita_accuracy = (temp_ita_accuracy/len_test_data)*100\n",
    "    print(f\"Model accuracy of Spanish Trigrams is {tri_spa_accuracy}%.\")\n",
    "    print(f\"Model accuracy of Italian Trigrams is {tri_ita_accuracy}%.\")\n",
    "\n",
    "    \n",
    "tri_spanish_italian(italian_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
