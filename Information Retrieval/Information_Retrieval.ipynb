{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Information_Retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVe3gqxVzHKY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### We have a small set of data in the form of tweets. Each line in the file begins with a document ID, followed by the text of the tweet. Implementing a function to create an inverted index of these documents.\n",
        "\n",
        "Reference: https://nlp.stanford.edu/IR-book/html/htmledition/a-first-take-at-building-an-inverted-index-1.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OxRMKbijWhn",
        "colab_type": "code",
        "outputId": "7c6e0f79-3842-4f77-e950-f6f2752a6e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Mounting the Drive to access the text file in the same folder\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzQn7JpVs3lQ",
        "colab_type": "code",
        "outputId": "5e1ac43f-eb5e-48fe-e4aa-0af02f346551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# Importing required libraries\n",
        "\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8ncs1sKi4xp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the Twitter corpus from the text file\n",
        "\n",
        "\n",
        "f = open('/content/drive/My Drive/Colab Notebooks/Copy_NLP_Assign_4/tweets_corpus.txt', 'r', encoding = \"utf-8\")\n",
        "tweet_corpus = f.read()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buSuZ07fjXNO",
        "colab_type": "code",
        "outputId": "62f72429-932b-420a-f650-ae8a4aebd8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tweet_corpus  # Printing the corpus"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"81499877556760576\\tBandaging up my paper-cuts , having cheesecake for dinner , and calling it a night . We're doin it big here in NYC .\\n81500716438523904\\tI haven't had any krispy kremes or strawberry trifles since I started gym * cries *\\n81503002321616896\\tBacon/cheddar slider topped w/fried egg & Blue cheese slider topped w/avocado & purple cherokee tomato\\n81507775422791680\\tNacho w/ cheese on my shirt ! Uggghhh\\n81534165975171072\\tyou aint nuffin but a piece of cheese without the corners .. in other words , you will never be a slice . BITCH .\\n81535634019323904\\tTAG_USERNAME TAG_USERNAME Mmmm ... cheese ... dreaming of a squirrel burger with cheese !\\n81577509950459904\\tMmmm cheese\\n81582996083326976\\tTAG_USERNAME 1st off I'm like 1 year younger than u , 2nd age is just a number , 3rd ima cater ur wedding wit patty n cheese\\n81587643376336896\\tRT TAG_USERNAME : I want a steak and cheese egg roll right now .\\n81600113016971264\\tthink imma eat some cheesecake befor i lay down ... Havent had it in a while\\n81623945064890368\\tA mixed one mostly strawberry peach little white cherry dr pepper or coke etc\\n81644157432643584\\tMy stomach was yelling at me telling me to get my ass up nd get somethin to eat . So I went nd got cheesesticks nd a waterbottle .\\n81656304107651072\\tchocolate mint , cookies & cream , very berry strawberry , and chocolate caramel~ all blend perfectly in my mouth\\n81673244926685184\\tI think I want some cheese eggs and pancakes ... but will I cook ? Where's my gf . This ain't right to make such a hard decision\\n81715158593966080\\tTAG_USERNAME Totally . It's also good on fish , chicken , veggies . Oh , and desserts . Drizzling it over strawberry tarts for a party tonight ! -C\\n81716618236928000\\tTAG_USERNAME I want to get the strawberry cheesecake and candy bear . When does the 2 for $ 38 expire ?\\n81736742478155778\\tMade myself some scrambled eggs with cheese and bacon bits\\n81842384404623360\\tTAG_USERNAME you could try it but its not great haha if i could recommend , get the strawberry one thats ma fave !\\n81844590625304576\\tTAG_USERNAME then im 100% pure strawberry flavoured hmmm tasty\\n82461950231064576\\tjust had the most beautiful pink coloured raspberry , strawberry & banana smoothie\\n82650970722533376\\tI went swimming , then ate asparagus bacon egg cheese biscuit goodness , then watched Date Night . It was ... it was good . TAG_FINAL_HASHTAGS\\n85032815321825280\\tCheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\\n85094773555335168\\tTAG_HASHTAGS using cream cheese icing on chocolate cake .. just use vanilla or strawberry\\n86441828815089664\\tRT TAG_USERNAME : RT TAG_USERNAME : Pancakes , bacon , eggs w/ cheese , & hashbrown casserole on deck TAG_HASHTAGS < ~i want sum !!!!! Ok it's good too\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWzProWJtjgU",
        "colab_type": "code",
        "outputId": "99d1898e-53e2-4a8a-ac45-b061d3362679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(str(81499877556760576))  # Length of doc id's is 17"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4gPKLQ5kDpX",
        "colab_type": "code",
        "outputId": "38cc3bf4-45a4-48ca-f078-adbe416c7429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Document ID's from the Twitter corpus is taken out\n",
        "# with regex and saved in a list\n",
        "\n",
        "\n",
        "doc_ids = re.findall(r'\\d{17}', tweet_corpus)\n",
        "doc_ids[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['81499877556760576', '81500716438523904', '81503002321616896']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCMcpYjHrJx-",
        "colab_type": "code",
        "outputId": "e72c2eaa-1a77-4cc7-cfc3-01a696891428",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Tweets from the corpus is taken out\n",
        "# and saved in a list\n",
        "\n",
        "\n",
        "tweets = re.findall(r'\\t(.*?)\\n', tweet_corpus)\n",
        "tweets[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Bandaging up my paper-cuts , having cheesecake for dinner , and calling it a night . We're doin it big here in NYC .\",\n",
              " \"I haven't had any krispy kremes or strawberry trifles since I started gym * cries *\",\n",
              " 'Bacon/cheddar slider topped w/fried egg & Blue cheese slider topped w/avocado & purple cherokee tomato']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNc-3yvjs5xR",
        "colab_type": "code",
        "outputId": "3000efd5-d3ea-438a-bff9-7ea74b29ab48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# Preprocessing text\n",
        "\n",
        "\n",
        "# Importing libraries\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem import PorterStemmer #Stemming\n",
        "\n",
        "def tweet_processor(tweets):\n",
        "  # Removing Punctuations\n",
        "  no_punc = [re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@\\\\^_`{|}~]', \" \", word) for word in tweets]\n",
        "\n",
        "  # Tokenizing\n",
        "  tweet_token = []\n",
        "  for tweet in no_punc:\n",
        "    tweet_token.append(word_tokenize(tweet))\n",
        "\n",
        "  # Removing stop words\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "\n",
        "  # Removing stop words and sentences less than 3 words\n",
        "  # and lowering text \n",
        "  filtered_tweets = []\n",
        "  for tweet in tweet_token:\n",
        "    filtered_sent = []\n",
        "    for w in tweet:\n",
        "      if w not in stop_words and (len(w) >= 3):\n",
        "        filtered_sent.append(w.lower())\n",
        "    filtered_tweets.append(filtered_sent)\n",
        "  return filtered_tweets\n",
        "\n",
        "filtered_tweets = tweet_processor(tweets)\n",
        "filtered_tweets[:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['bandaging',\n",
              "  'paper',\n",
              "  'cuts',\n",
              "  'cheesecake',\n",
              "  'dinner',\n",
              "  'calling',\n",
              "  'night',\n",
              "  'doin',\n",
              "  'big',\n",
              "  'nyc'],\n",
              " ['krispy',\n",
              "  'kremes',\n",
              "  'strawberry',\n",
              "  'trifles',\n",
              "  'since',\n",
              "  'started',\n",
              "  'gym',\n",
              "  'cries']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II_wt7XyzSz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stemming tokens\n",
        "\n",
        "# stem_tweet = []\n",
        "# ps = PorterStemmer()\n",
        "# for tweet in tweet_token:\n",
        "#   stems = []\n",
        "#   for w in tweet:\n",
        "#     stems.append(ps.stem(w))\n",
        "#   stem_tweet.append(stems)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61Jv28PyHoBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Term frequency calculation\n",
        "\n",
        "# tf = []\n",
        "# for tweet in filtered_tweets:\n",
        "#   tf.append([(x, tweet.count(x)/len(tweet)) for x in set(tweet)])\n",
        "\n",
        "# tf[:2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IIQ9WUGMqh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inverse Document Frequency\n",
        "\n",
        "# docs_count = len(doc_ids)\n",
        "\n",
        "# number of documents containing the word \n",
        "# idf = []\n",
        "# for tweet in filtered_tweets:\n",
        "#   count_doc = []\n",
        "#   for w in tweet:\n",
        "#     count_doc.append(len(inverted_index[w])/docs_count)\n",
        "#   idf.append(count_doc[1:])\n",
        "\n",
        "# idf[:2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDNXVqNhDiQ0",
        "colab_type": "code",
        "outputId": "dadbfe36-8635-4c54-bcdf-8437e2a4c47f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# Tabulating data\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df[\"Doc_ids\"] = doc_ids\n",
        "df[\"Tweets\"] = filtered_tweets\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc_ids</th>\n",
              "      <th>Tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81499877556760576</td>\n",
              "      <td>[bandaging, paper, cuts, cheesecake, dinner, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>81500716438523904</td>\n",
              "      <td>[krispy, kremes, strawberry, trifles, since, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>81503002321616896</td>\n",
              "      <td>[bacon, cheddar, slider, topped, fried, egg, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>81507775422791680</td>\n",
              "      <td>[nacho, cheese, shirt, uggghhh]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>81534165975171072</td>\n",
              "      <td>[aint, nuffin, piece, cheese, without, corners...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Doc_ids                                             Tweets\n",
              "0  81499877556760576  [bandaging, paper, cuts, cheesecake, dinner, c...\n",
              "1  81500716438523904  [krispy, kremes, strawberry, trifles, since, s...\n",
              "2  81503002321616896  [bacon, cheddar, slider, topped, fried, egg, b...\n",
              "3  81507775422791680                    [nacho, cheese, shirt, uggghhh]\n",
              "4  81534165975171072  [aint, nuffin, piece, cheese, without, corners..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c518lA1euUtk",
        "colab_type": "code",
        "outputId": "728363b4-e758-4d50-b50e-c654bd7b4e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "inverted_index = defaultdict(list)\n",
        "\n",
        "i = 0\n",
        "for tweet in filtered_tweets:\n",
        "  for w in tweet:\n",
        "    inverted_index[w].append(doc_ids[i])\n",
        "  i+=1\n",
        "\n",
        "# Printing inverted index for a few words\n",
        "dict(list(inverted_index.items())[:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bacon': ['81503002321616896',\n",
              "  '81736742478155778',\n",
              "  '82650970722533376',\n",
              "  '85032815321825280',\n",
              "  '86441828815089664'],\n",
              " 'bandaging': ['81499877556760576'],\n",
              " 'big': ['81499877556760576'],\n",
              " 'calling': ['81499877556760576'],\n",
              " 'cheddar': ['81503002321616896'],\n",
              " 'cheesecake': ['81499877556760576', '81600113016971264', '81716618236928000'],\n",
              " 'cries': ['81500716438523904'],\n",
              " 'cuts': ['81499877556760576'],\n",
              " 'dinner': ['81499877556760576'],\n",
              " 'doin': ['81499877556760576'],\n",
              " 'gym': ['81500716438523904'],\n",
              " 'kremes': ['81500716438523904'],\n",
              " 'krispy': ['81500716438523904'],\n",
              " 'night': ['81499877556760576', '82650970722533376'],\n",
              " 'nyc': ['81499877556760576'],\n",
              " 'paper': ['81499877556760576'],\n",
              " 'since': ['81500716438523904'],\n",
              " 'started': ['81500716438523904'],\n",
              " 'strawberry': ['81500716438523904',\n",
              "  '81623945064890368',\n",
              "  '81656304107651072',\n",
              "  '81715158593966080',\n",
              "  '81716618236928000',\n",
              "  '81842384404623360',\n",
              "  '81844590625304576',\n",
              "  '82461950231064576',\n",
              "  '85094773555335168'],\n",
              " 'trifles': ['81500716438523904']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDqv9-ZNzVb5",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### Writing a function to implement the merge algorithm. Your code should allow intersecting the postings of two terms, as well as process simple Boolean queries. When there are multiple query terms, make sure that your algorithm uses the optimization described in Manning book of performing the most restrictive intersection first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ_KJHpLDDKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AND(posting1, posting2):\n",
        "    p1 = 0\n",
        "    p2 = 0\n",
        "    result = list()\n",
        "    while p1 < len(posting1) and p2 < len(posting2):\n",
        "        if posting1[p1] == posting2[p2]:\n",
        "            result.append(posting1[p1])\n",
        "            p1 += 1\n",
        "            p2 += 1\n",
        "        elif posting1[p1] > posting2[p2]:\n",
        "            p2 += 1\n",
        "        else:\n",
        "            p1 += 1\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9xeXKQo3Hek",
        "colab_type": "code",
        "outputId": "5ad7ba57-f313-434e-9be9-e889919c3f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "and_ans = AND(inverted_index[\"cheddar\"], inverted_index[\"cheese\"])\n",
        "\n",
        "print(\"The documents with \\\"cheese\\\" and \\\"cheddar\\\" in them are:\")\n",
        "print(and_ans)\n",
        "print(\"\\n\")\n",
        "print(\"Document(s) Contents:\")\n",
        "for ans in and_ans:\n",
        "  print(tweets[doc_ids.index(ans)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The documents with \"cheese\" and \"cheddar\" in them are:\n",
            "['81503002321616896']\n",
            "\n",
            "\n",
            "Document(s) Contents:\n",
            "Bacon/cheddar slider topped w/fried egg & Blue cheese slider topped w/avocado & purple cherokee tomato\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDkp2wUjzn-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def OR(posting1, posting2):\n",
        "    p1 = 0\n",
        "    p2 = 0\n",
        "    result = list()\n",
        "    while p1 < len(posting1) and p2 < len(posting2):\n",
        "        if posting1[p1] == posting2[p2]:\n",
        "            result.append(posting1[p1])\n",
        "            p1 += 1\n",
        "            p2 += 1\n",
        "        elif posting1[p1] > posting2[p2]:\n",
        "            result.append(posting2[p2])\n",
        "            p2 += 1\n",
        "        else:\n",
        "            result.append(posting1[p1])\n",
        "            p1 += 1\n",
        "    while p1 < len(posting1):\n",
        "        result.append(posting1[p1])\n",
        "        p1 += 1\n",
        "    while p2 < len(posting2):\n",
        "        result.append(posting2[p2])\n",
        "        p2 += 1\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNwF-ldPzrfE",
        "colab_type": "code",
        "outputId": "006b8a9f-c4bc-453f-b44e-7d3468e1b925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "or_ans = OR(inverted_index[\"cheddar\"], inverted_index[\"cookies\"])\n",
        "\n",
        "print(\"The documents with \\\"cookies\\\" or \\\"cheddar\\\" in them are:\")\n",
        "print(or_ans)\n",
        "print(\"\\n\")\n",
        "print(\"Document(s) Contents:\")\n",
        "for ans in or_ans:\n",
        "  print(tweets[doc_ids.index(ans)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The documents with \"cookies\" or \"cheddar\" in them are:\n",
            "['81503002321616896', '81656304107651072']\n",
            "\n",
            "\n",
            "Document(s) Contents:\n",
            "Bacon/cheddar slider topped w/fried egg & Blue cheese slider topped w/avocado & purple cherokee tomato\n",
            "chocolate mint , cookies & cream , very berry strawberry , and chocolate caramel~ all blend perfectly in my mouth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BvJUMBcafy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"query_splitter\" function takes in a string\n",
        "# and returns operators and terms lists\n",
        "\n",
        "\n",
        "def query_splitter(words):\n",
        "\n",
        "  # Initializing terms and operations for an individual query\n",
        "  temp_terms = []\n",
        "  temp_op = []\n",
        "\n",
        "  # Iterating through every individual query term\n",
        "  for q in words:\n",
        "\n",
        "    # All the non-operation terms are stored in \"temp_terms\" list\n",
        "    # and the operation is stored in \"temp_op\" variable\n",
        "    if q != \"and\" and q != \"or\":\n",
        "        temp_terms.append(q)\n",
        "    else:\n",
        "        temp_op.append(q)\n",
        "  return temp_terms, temp_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4tjdeFiWjMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"boolean_querying\" function takes boolean queries \n",
        "# and returns documents according to the queries\n",
        "\n",
        "\n",
        "def boolean_querying(query):\n",
        "\n",
        "  # Lowering text\n",
        "  temp_query = query.lower() \n",
        "\n",
        "  # Splitting to get the words\n",
        "  split_query = temp_query.split()\n",
        "\n",
        "  # Storing individual queries between two parentheses in a list\n",
        "  individual_query = re.findall(r'\\((.*?)\\)', temp_query)\n",
        "\n",
        "  # Storing additional operations that are outside the parentheses in a list\n",
        "  additional_op = re.findall(r'\\) (.*?) \\(', temp_query)\n",
        "\n",
        "\n",
        "  # -------- CASE 1: --------\n",
        "  # Retrieving results for queries which have parantheses\n",
        "  # Queries like: \"(egg and cheese) or (cookies and cheesecake)\"\n",
        "  if individual_query != []:\n",
        "\n",
        "    # Initializing answers to store the individual query results\n",
        "    answers = []\n",
        "\n",
        "    # Iterating through individual query list\n",
        "    for q in individual_query:\n",
        "\n",
        "      temp_terms, temp_op = query_splitter(q.split())\n",
        "\n",
        "      # temp_terms = []\n",
        "      # temp_op = \"\"\n",
        "      # for w in q.split():\n",
        "      #   if w != \"and\" and w != \"or\":\n",
        "      #       temp_terms.append(w)\n",
        "      #   else:\n",
        "      #       temp_op = w\n",
        "\n",
        "      # According to the value stored in \"temp_op\", AND() or OR() is called\n",
        "      # Answers are stored in \"answers\" list\n",
        "      if temp_op[0] == \"and\":\n",
        "        answers.append(AND(inverted_index[temp_terms[0]], inverted_index[temp_terms[1]]))\n",
        "      else:\n",
        "        answers.append(OR(inverted_index[temp_terms[0]], inverted_index[temp_terms[1]]))\n",
        "\n",
        "    \n",
        "    # Additional operation(outside paranthesis) is performed \n",
        "\n",
        "    # Initialization\n",
        "    i = 0\n",
        "    answer = []\n",
        "\n",
        "    # For every operation in \"additional_op\" variable\n",
        "    # the corresponding answers are retrived from \"answers\"\n",
        "    for o in additional_op: \n",
        "      if o == 'and':\n",
        "        answer.append(AND(answers[i], answers[i+1]))\n",
        "      else: \n",
        "        answer.append(OR(answers[i], answers[i+1]))\n",
        "      i+=1\n",
        "\n",
        "    # \"answer\" variable contains all the documents from the query \n",
        "    return answer\n",
        "  # -------------------------\n",
        "\n",
        "\n",
        "\n",
        "  # -------- CASE 2: --------\n",
        "  # Retrieving results for queries which have more than 3 words in them\n",
        "  # Queries like: \"egg and cheese and cheesecake\"\n",
        "  elif (len(split_query)>3):\n",
        "\n",
        "    temp_terms, temp_op = query_splitter(split_query)\n",
        "\n",
        "    # Optimizing same boolean operator search\n",
        "    # If there are two \"AND\"s in the search\n",
        "    # Or two \"OR\"s in the search\n",
        "    if temp_op[0] == temp_op[1]:\n",
        "      posting_len = []\n",
        "      for n in temp_terms:\n",
        "        posting_len.append(len(inverted_index[n]))\n",
        "      temp_terms = [x for _,x in sorted(zip(posting_len,temp_terms))]\n",
        "\n",
        "    p1 = inverted_index[temp_terms[0]]\n",
        "    i = 1; j = 0\n",
        "    while i < len(temp_terms):\n",
        "      p2 = inverted_index[temp_terms[i]]\n",
        "      if temp_op == \"and\":\n",
        "        p1 = AND(p1, p2)\n",
        "      else:\n",
        "        p1 = OR(p1, p2)\n",
        "      i+=1; j+=1\n",
        "    answer = p1\n",
        "    return answer\n",
        "  # -------------------------\n",
        "\n",
        "\n",
        "\n",
        "  # -------- CASE 3: --------\n",
        "  # Retrieving results for queries which have more than 3 words in them\n",
        "  # Queries like: \"egg and cheese\"\n",
        "  else: \n",
        "\n",
        "    terms, op = query_splitter(split_query)\n",
        "    if op[0] == \"and\":\n",
        "      answer = AND(inverted_index[terms[0]], inverted_index[terms[1]])\n",
        "    else:\n",
        "      answer = OR(inverted_index[terms[0]], inverted_index[terms[1]])\n",
        "    return answer\n",
        "  # -------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SQz8TalyM3n",
        "colab_type": "code",
        "outputId": "7f5291d1-f9b6-4bd8-b8d7-67631680c432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "boolean_querying(\"egg and cheese\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['81503002321616896', '81587643376336896', '82650970722533376']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6AQ2xXzywwo",
        "colab_type": "code",
        "outputId": "08caee96-aa29-4b9b-dd45-0c3f831b287c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "boolean_querying(\"egg and cheese and cheesecake\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['81499877556760576',\n",
              " '81503002321616896',\n",
              " '81507775422791680',\n",
              " '81534165975171072',\n",
              " '81535634019323904',\n",
              " '81535634019323904',\n",
              " '81577509950459904',\n",
              " '81582996083326976',\n",
              " '81587643376336896',\n",
              " '81600113016971264',\n",
              " '81673244926685184',\n",
              " '81716618236928000',\n",
              " '81736742478155778',\n",
              " '82650970722533376',\n",
              " '85032815321825280',\n",
              " '85094773555335168',\n",
              " '86441828815089664']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3utqAyXHzHCS",
        "colab_type": "code",
        "outputId": "4e1dc3ac-6e34-4c87-a840-ce2f383e478a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "boolean_querying(\"egg or cheese\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['81503002321616896',\n",
              " '81507775422791680',\n",
              " '81534165975171072',\n",
              " '81535634019323904',\n",
              " '81535634019323904',\n",
              " '81577509950459904',\n",
              " '81582996083326976',\n",
              " '81587643376336896',\n",
              " '81673244926685184',\n",
              " '81736742478155778',\n",
              " '82650970722533376',\n",
              " '85032815321825280',\n",
              " '85094773555335168',\n",
              " '86441828815089664']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS9co7MkzKGq",
        "colab_type": "code",
        "outputId": "5c0c07fe-33f5-4361-e49d-10d31602568c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "boolean_querying(\"egg and cheese or cookies\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['81503002321616896',\n",
              " '81507775422791680',\n",
              " '81534165975171072',\n",
              " '81535634019323904',\n",
              " '81535634019323904',\n",
              " '81577509950459904',\n",
              " '81582996083326976',\n",
              " '81587643376336896',\n",
              " '81656304107651072',\n",
              " '81673244926685184',\n",
              " '81736742478155778',\n",
              " '82650970722533376',\n",
              " '85032815321825280',\n",
              " '85094773555335168',\n",
              " '86441828815089664']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLJVZbwly0Rz",
        "colab_type": "code",
        "outputId": "d27fee39-4cab-4f27-d971-e6c15d486a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "boolean_querying(\"(egg and cheese) or (cookies and cream)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['81503002321616896',\n",
              "  '81587643376336896',\n",
              "  '81656304107651072',\n",
              "  '82650970722533376']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV3xiB9qI5sp",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### Extending the system from above to perform simple TF-IDF scoring of the retrieved results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi76oW8orfJZ",
        "colab_type": "code",
        "outputId": "030c641b-3454-4175-c1f3-bf6a74a18138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# Term Frequency Calculation\n",
        "# Reference: https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
        "\n",
        "tf = {}\n",
        "\n",
        "for i in range(len(filtered_tweets)):\n",
        "    for w in filtered_tweets[i]:\n",
        "      try:\n",
        "          tf[w].add(i)\n",
        "      except:\n",
        "          tf[w] = {i}\n",
        "for t in tf:\n",
        "  tf[t]=len(tf[t])\n",
        "\n",
        "\n",
        "# Printing term frequency for a few words\n",
        "dict(list(tf.items())[:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bacon': 5,\n",
              " 'bandaging': 1,\n",
              " 'big': 1,\n",
              " 'calling': 1,\n",
              " 'cheddar': 1,\n",
              " 'cheesecake': 3,\n",
              " 'cries': 1,\n",
              " 'cuts': 1,\n",
              " 'dinner': 1,\n",
              " 'doin': 1,\n",
              " 'gym': 1,\n",
              " 'kremes': 1,\n",
              " 'krispy': 1,\n",
              " 'night': 2,\n",
              " 'nyc': 1,\n",
              " 'paper': 1,\n",
              " 'since': 1,\n",
              " 'started': 1,\n",
              " 'strawberry': 9,\n",
              " 'trifles': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clW20bwn1Gdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tf-idf\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "tf_idf = {}\n",
        "\n",
        "for i in range(len(filtered_tweets)):\n",
        "    words = filtered_tweets[i]\n",
        "    counter = Counter(words)\n",
        "    words_count = len(words)\n",
        "    for t in np.unique(words):\n",
        "        tf = counter[t]/words_count\n",
        "        df = tf[t]\n",
        "        idf = np.log(len(tweets)/(df+1))\n",
        "        tf_idf[doc_ids[i], t] = tf*idf\n",
        "\n",
        "tf_idf"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}