{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVe3gqxVzHKY"
   },
   "source": [
    "##### We have a small set of data in the form of tweets. Each line in the file begins with a document ID, followed by the text of the tweet. Implementing a function to create an inverted index of these documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "0OxRMKbijWhn",
    "outputId": "b3bfbc70-a962-4b97-ccc3-cbbafea857c5"
   },
   "outputs": [],
   "source": [
    "# Mounting the Drive to access the text file in the same folder\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "YzQn7JpVs3lQ",
    "outputId": "42709f8d-98bc-4085-c9c2-77df4e632c4c"
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8ncs1sKi4xp"
   },
   "outputs": [],
   "source": [
    "# Reading the Twitter corpus from the text file\n",
    "\n",
    "\n",
    "f = open('/content/drive/My Drive/Colab Notebooks/NLP_Assignment_4/tweets_corpus.txt', 'r', encoding = \"utf-8\")\n",
    "tweet_corpus = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "buSuZ07fjXNO",
    "outputId": "34417cb9-f8d7-4246-b441-ff1d02df0422"
   },
   "outputs": [],
   "source": [
    "tweet_corpus  # Printing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yWzProWJtjgU",
    "outputId": "8da77f3d-65b5-4073-fdbf-392ae6b2a0e6"
   },
   "outputs": [],
   "source": [
    "len(str(81499877556760576))  # Length of doc id's is 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r4gPKLQ5kDpX",
    "outputId": "e6bb1ae3-d091-468b-cb74-15ec3b653e41"
   },
   "outputs": [],
   "source": [
    "# Document ID's from the Twitter corpus is taken out\n",
    "# with regex and saved in a list\n",
    "\n",
    "\n",
    "doc_ids = re.findall(r'\\d{17}', tweet_corpus)\n",
    "doc_ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "RCMcpYjHrJx-",
    "outputId": "770e6ded-89c7-45a4-e78f-a75a5d0941df"
   },
   "outputs": [],
   "source": [
    "# Tweets from the corpus is taken out\n",
    "# and saved in a list\n",
    "\n",
    "\n",
    "tweets = re.findall(r'\\t(.*?)\\n', tweet_corpus)\n",
    "tweets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "yNc-3yvjs5xR",
    "outputId": "d9603e1c-3f23-4f2c-f881-569d1ac9e8ff"
   },
   "outputs": [],
   "source": [
    "# Preprocessing text\n",
    "\n",
    "\n",
    "# Importing libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer #Stemming\n",
    "\n",
    "\n",
    "# Removing Punctuations\n",
    "no_punc = [re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@\\\\^_`{|}~]', \" \", word) for word in tweets]\n",
    "\n",
    "\n",
    "# Tokenizing\n",
    "tweet_token = []\n",
    "for tweet in no_punc:\n",
    "  tweet_token.append(word_tokenize(tweet))\n",
    "\n",
    "\n",
    "# Removing stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "\n",
    "# Removing stop words and sentences less than 3 words\n",
    "# and lowering text \n",
    "filtered_tweets = []\n",
    "for tweet in tweet_token:\n",
    "  filtered_sent = []\n",
    "  for w in tweet:\n",
    "    if w not in stop_words and (len(w) >= 3):\n",
    "      filtered_sent.append(w.lower())\n",
    "  filtered_tweets.append(filtered_sent)\n",
    "\n",
    "\n",
    "filtered_tweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61Jv28PyHoBR"
   },
   "outputs": [],
   "source": [
    "# Term frequency calculation\n",
    "\n",
    "# tf = []\n",
    "# for tweet in filtered_tweets:\n",
    "#   tf.append([(x, tweet.count(x)/len(tweet)) for x in set(tweet)])\n",
    "\n",
    "# tf[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4IIQ9WUGMqh4"
   },
   "outputs": [],
   "source": [
    "# Inverse Document Frequency\n",
    "\n",
    "# docs_count = len(doc_ids)\n",
    "\n",
    "\n",
    "# number of documents containing the word \n",
    "# idf = []\n",
    "# for tweet in filtered_tweets:\n",
    "#   count_doc = []\n",
    "#   for w in tweet:\n",
    "#     count_doc.append(len(inverted_index[w])/docs_count)\n",
    "#   idf.append(count_doc[1:])\n",
    "\n",
    "# idf[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "II_wt7XyzSz_"
   },
   "outputs": [],
   "source": [
    "# Stemming tokens\n",
    "\n",
    "# stem_tweet = []\n",
    "# ps = PorterStemmer()\n",
    "# for tweet in tweet_token:\n",
    "#   stems = []\n",
    "#   for w in tweet:\n",
    "#     stems.append(ps.stem(w))\n",
    "#   stem_tweet.append(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "mDNXVqNhDiQ0",
    "outputId": "f79230a0-0166-4124-ccf4-cf9416131ee4"
   },
   "outputs": [],
   "source": [
    "# Showing the data\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"Doc_ids\"] = doc_ids\n",
    "df[\"Tweets\"] = filtered_tweets\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c518lA1euUtk",
    "outputId": "e3585fc5-73c5-473f-d2f7-3db05dd66e60"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inverted_index = defaultdict(list)\n",
    "\n",
    "i = 0\n",
    "for tweet in filtered_tweets:\n",
    "  for w in tweet:\n",
    "    inverted_index[w].append(doc_ids[i])\n",
    "  i+=1\n",
    "\n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDqv9-ZNzVb5"
   },
   "source": [
    "<br>\n",
    "\n",
    "#####  Writing a function to implement the merge algorithm. Your code should allow intersecting the postings of two terms, as well as process simple Boolean queries. When there are multiple query terms, make sure that your algorithm uses the optimization described in Manning book of performing the most restrictive intersection first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQ_KJHpLDDKT"
   },
   "outputs": [],
   "source": [
    "def AND(posting1, posting2):\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    result = list()\n",
    "    while p1 < len(posting1) and p2 < len(posting2):\n",
    "        if posting1[p1] == posting2[p2]:\n",
    "            result.append(posting1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        elif posting1[p1] > posting2[p2]:\n",
    "            p2 += 1\n",
    "        else:\n",
    "            p1 += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "k9xeXKQo3Hek",
    "outputId": "b171a5e6-cc19-49dc-f64c-07a4ca1dbb4d"
   },
   "outputs": [],
   "source": [
    "and_ans = AND(inverted_index[\"cheddar\"], inverted_index[\"cheese\"])\n",
    "\n",
    "print(\"The documents with \\\"cheese\\\" and \\\"cheddar\\\" in them are:\")\n",
    "print(and_ans)\n",
    "print(\"\\n\")\n",
    "print(\"Document(s) Contents:\")\n",
    "for ans in and_ans:\n",
    "  print(tweets[doc_ids.index(ans)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDkp2wUjzn-t"
   },
   "outputs": [],
   "source": [
    "def OR(posting1, posting2):\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    result = list()\n",
    "    while p1 < len(posting1) and p2 < len(posting2):\n",
    "        if posting1[p1] == posting2[p2]:\n",
    "            result.append(posting1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        elif posting1[p1] > posting2[p2]:\n",
    "            result.append(posting2[p2])\n",
    "            p2 += 1\n",
    "        else:\n",
    "            result.append(posting1[p1])\n",
    "            p1 += 1\n",
    "    while p1 < len(posting1):\n",
    "        result.append(posting1[p1])\n",
    "        p1 += 1\n",
    "    while p2 < len(posting2):\n",
    "        result.append(posting2[p2])\n",
    "        p2 += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "kNwF-ldPzrfE",
    "outputId": "f653a6d8-b2e8-402a-8db5-18465302d05e"
   },
   "outputs": [],
   "source": [
    "or_ans = OR(inverted_index[\"cheddar\"], inverted_index[\"cookies\"])\n",
    "\n",
    "print(\"The documents with \\\"cookies\\\" or \\\"cheddar\\\" in them are:\")\n",
    "print(or_ans)\n",
    "print(\"\\n\")\n",
    "print(\"Document(s) Contents:\")\n",
    "for ans in or_ans:\n",
    "  print(tweets[doc_ids.index(ans)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4tjdeFiWjMk"
   },
   "outputs": [],
   "source": [
    "def querying(query):\n",
    "  temp_query = query.lower()\n",
    "  individual_query = re.findall(r'\\((.*?)\\)', temp_query)\n",
    "  additional_op = re.findall(r'\\) (.*?) \\(', temp_query)\n",
    "\n",
    "  operation = []\n",
    "  if individual_query != []:\n",
    "    answers = []\n",
    "    for q in individual_query:\n",
    "      temp_terms = []\n",
    "      temp_op = \"\"\n",
    "      for w in q.split():\n",
    "        if w != \"and\" and w != \"or\":\n",
    "            temp_terms.append(w)\n",
    "        else:\n",
    "            temp_op = w\n",
    "      if temp_op == \"and\":\n",
    "        answers.append(AND(inverted_index[temp_terms[0]], inverted_index[temp_terms[1]]))\n",
    "      else:\n",
    "        answers.append(OR(inverted_index[temp_terms[0]], inverted_index[temp_terms[1]]))\n",
    "    i = 0\n",
    "    answer = []\n",
    "    for o in additional_op: \n",
    "      if o == 'and':\n",
    "        answer.append(AND(answers[i], answers[i+1]))\n",
    "      else: \n",
    "        answer.append(OR(answers[i], answers[i+1]))\n",
    "      i+=1\n",
    "    return answer\n",
    "  elif (len(temp_query.split())>3):\n",
    "    for q in temp_query.split():\n",
    "      temp_terms = []\n",
    "      temp_op = []\n",
    "      if q != \"and\" and q != \"or\":\n",
    "          temp_terms.append(q)\n",
    "      else:\n",
    "          temp_op.append(q)\n",
    "    p1 = inverted_index[temp_terms[0]]\n",
    "    i = 1; j = 0\n",
    "    while i < len(temp_terms):\n",
    "      p2 = inverted_index[temp_terms[i]]\n",
    "      if temp_op == \"and\":\n",
    "        p1 = AND(inverted_index[temp_terms[p1]], inverted_index[temp_terms[p2]])\n",
    "      else:\n",
    "        p1 = OR(inverted_index[temp_terms[p1]], inverted_index[temp_terms[p2]])\n",
    "      i+=1; j+=1\n",
    "    answer = p1\n",
    "    return answer\n",
    "  else: \n",
    "    terms = []\n",
    "    op = \"\"\n",
    "    for w in temp_query.split():\n",
    "        if w != \"and\" and w != \"or\":\n",
    "            terms.append(w)\n",
    "        else:\n",
    "            op = w\n",
    "    if op == \"and\":\n",
    "      answer = AND(inverted_index[terms[0]], inverted_index[terms[1]])\n",
    "    else:\n",
    "      answer = OR(inverted_index[terms[0]], inverted_index[terms[1]])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-SQz8TalyM3n",
    "outputId": "d8d361c1-1cde-4db1-b167-98d8377c478b"
   },
   "outputs": [],
   "source": [
    "querying(\"egg and cheese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p6AQ2xXzywwo",
    "outputId": "2a9b4a75-4463-43a5-b928-6901a09861f2"
   },
   "outputs": [],
   "source": [
    "querying(\"egg and cheese and cheesecake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "3utqAyXHzHCS",
    "outputId": "502e250c-e93b-4856-bb14-572d3a709afe"
   },
   "outputs": [],
   "source": [
    "querying(\"egg or cheese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SS9co7MkzKGq",
    "outputId": "1e934fcf-4d78-4259-a39d-347d73315161"
   },
   "outputs": [],
   "source": [
    "querying(\"egg and cheese or cookies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "eLJVZbwly0Rz",
    "outputId": "70a1828d-9149-4126-87c9-6da583b915dd"
   },
   "outputs": [],
   "source": [
    "querying(\"(egg and cheese) or (cookies and cream)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "c8OIVkv2X643",
    "outputId": "0a411821-01d0-4c52-98bd-a099c87e70ad"
   },
   "outputs": [],
   "source": [
    "# Code for optimization\n",
    "\n",
    "# posting_len = []\n",
    "# for n in terms:\n",
    "#   posting_len.append(len(inverted_index[n]))\n",
    "\n",
    "# sorted_terms_list = [x for _,x in sorted(zip(posting_len,terms))]\n",
    "# print(sorted_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QV3xiB9qI5sp"
   },
   "source": [
    "##### Extend the system from Problem 2 to perform simple TF-IDF scoring of the retrieved results. There is no need to worry about any weight normalizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qi76oW8orfJZ",
    "outputId": "37b094c8-e10c-4e07-8678-7cd3f5b4b8c7"
   },
   "outputs": [],
   "source": [
    "# Term Frequency Calculation\n",
    "# Reference: https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "\n",
    "tf = {}\n",
    "\n",
    "for i in range(len(filtered_tweets)):\n",
    "    for w in filtered_tweets[i]:\n",
    "      try:\n",
    "          tf[w].add(i)\n",
    "      except:\n",
    "          tf[w] = {i}\n",
    "for t in tf:\n",
    "  tf[t]=len(tf[t])\n",
    "\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "clW20bwn1Gdh",
    "outputId": "f21abb05-7574-4947-8d66-49ac824ff457"
   },
   "outputs": [],
   "source": [
    "# Tf-idf\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(len(filtered_tweets)):\n",
    "    words = filtered_tweets[i]\n",
    "    counter = Counter(words)\n",
    "    words_count = len(words)\n",
    "    for t in np.unique(words):\n",
    "        tf = counter[t]/words_count\n",
    "        df = tf[t]\n",
    "        idf = np.log(len(tweets)/(df+1))\n",
    "        tf_idf[doc_ids[i], t] = tf*idf\n",
    "\n",
    "tf_idf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Information_Retrieval.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
